{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"image_names\": shape (100,), type \"|O\">"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "data = h5py.File('ds4.h5')\n",
    "data.create_dataset('images', \n",
    "                    shape=(100, 256, 256, 3), \n",
    "                    maxshape=(None, 256, 256, None),\n",
    "                    dtype=float)\n",
    " \n",
    "data.create_dataset('image_names', \n",
    "                    shape=(100,),\n",
    "                    maxshape=(None,),\n",
    "                    dtype=h5py.special_dtype(vlen=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"image_names\": shape (100,), type \"|O\">\n",
      "<HDF5 dataset \"images\": shape (100, 256, 256, 3), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "data['image_names'][:2] = ['some_randome_name.jpg', 'some_randome_name2.jpg']\n",
    "data['images'][:2] = np.ones((2, 256, 256, 3))\n",
    " \n",
    "# list available datasets in this file\n",
    "for ds in data:\n",
    "    print(data[ds])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_randome_name.jpg\n",
      "<HDF5 dataset \"image_names\": shape (200,), type \"|O\">\n"
     ]
    }
   ],
   "source": [
    "print(data['image_names'][0])\n",
    " \n",
    "images = data['image_names']\n",
    "images.resize(200, axis=0)\n",
    " \n",
    "print(data['image_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic structure in an h5py file is a dataset, we need to specify the desired shapes during creation. This step should be pretty self-explanatory. There is a small catch though, if you donâ€™t specify the max shape argument you will not be able to resize the dataset in the future. Due to some performance optimizations, this is ok if you are 100% sure you will not need to extend it.\n",
    "\n",
    "In our case, we set the max shape for axis=0 and axis=3 to None, this means that we will be able to resize this dataset without any limitations along that axis. So basically adding more images and/or adding more channels to each image.\n",
    "\n",
    "Lets create a simple class that will implement the indexer with buffering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import timeit\n",
    "import time\n",
    " \n",
    " \n",
    "class ImageIndexer(object):\n",
    "    def __init__(self, db_path, fixed_image_shape=(512, 512), buffer_size=200, num_of_images=100):\n",
    "        self.db = h5py.File(db_path, mode='w')\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_of_images = num_of_images\n",
    "        self.fixed_image_shape = fixed_image_shape\n",
    "        self.image_vector_db = None\n",
    "        self.image_id_db = None\n",
    "        #         self.db_index = None\n",
    "        self.idxs = {\"index\": 0}\n",
    " \n",
    "        self.image_vector_buffer = []\n",
    "        self.image_id_buffer = []\n",
    " \n",
    "    #         self.db_index_buffer = []\n",
    " \n",
    "    def __enter__(self):\n",
    "        print(\"indexing {} images\".format(self.num_of_images))\n",
    "        self.t0 = time.time()\n",
    "        return self\n",
    " \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.image_id_buffer:\n",
    "            print(\"writing last buffers\")\n",
    "            print(len(self.image_id_buffer))\n",
    " \n",
    "            self._write_buffer(self.image_id_db, self.image_id_buffer)\n",
    "            self._write_buffer(self.image_vector_db, self.image_vector_buffer)\n",
    " \n",
    "        print(\"closing h5 db\")\n",
    "        self.db.close()\n",
    "        print(\"indexing took {0}\".format(time.time() - self.t0))\n",
    " \n",
    "    @property\n",
    "    def image_vector_size(self):\n",
    "        if self.fixed_image_shape:\n",
    "            return self.fixed_image_shape[0] * self.fixed_image_shape[1]\n",
    "        else:\n",
    "            return None\n",
    " \n",
    "    def create_datasets(self):\n",
    " \n",
    "        IMG_ROWS, IMG_COLS, CHANN = self.fixed_image_shape\n",
    " \n",
    "        self.image_id_db = self.db.create_dataset(\n",
    "            \"image_ids\",\n",
    "            (self.num_of_images,),\n",
    "            maxshape=None,\n",
    "            dtype=h5py.special_dtype(vlen=str)\n",
    " \n",
    "        )\n",
    " \n",
    "        self.image_vector_db = self.db.create_dataset(\n",
    "            \"images\",\n",
    "            shape=(self.num_of_images,IMG_ROWS, IMG_COLS, CHANN),\n",
    "            dtype=\"float\"\n",
    "        )\n",
    " \n",
    "    def add(self, image_name, image_vector):\n",
    "        self.image_id_buffer.append(image_name)\n",
    "        self.image_vector_buffer.append(image_vector)\n",
    " \n",
    "        if None in (self.image_vector_db, self.image_id_db):\n",
    "            self.create_datasets()\n",
    " \n",
    "        if len(self.image_id_buffer) >= self.buffer_size:\n",
    "            self._write_buffer(self.image_id_db, self.image_id_buffer)\n",
    "            self._write_buffer(self.image_vector_db, self.image_vector_buffer)\n",
    " \n",
    "            # increment index\n",
    "            self.idxs['index'] += len(self.image_vector_buffer)\n",
    " \n",
    "            # clean buffers\n",
    "            self._clean_buffers()\n",
    " \n",
    "    def _write_buffer(self, dataset, buf):\n",
    "        print(\"Writing buffer {}\".format(dataset))\n",
    "        start = self.idxs['index']\n",
    "        end = len(buf)\n",
    "        dataset[start:start + end] = buf\n",
    " \n",
    "    def _clean_buffers(self):\n",
    "        self.image_id_buffer = []\n",
    "        self.image_vector_buffer = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage\n",
    "\n",
    "Since we implemented enter and exit methods we can use this class as a context manager, this will come handy to close the file at the end and write the last buffers at context-exit.\n",
    "\n",
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing 200 images\n",
      "closing h5 db\n",
      "indexing took 0.0006651878356933594\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'list_of_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-547393e86642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                   num_of_images=200) as imageindexer:\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmy_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mimage_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimageindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_of_images' is not defined"
     ]
    }
   ],
   "source": [
    "with ImageIndexer('my_dataset.h5', \n",
    "                  fixed_image_shape=(512, 512, 3), \n",
    "                  buffer_size=200, \n",
    "                  num_of_images=200) as imageindexer:\n",
    " \n",
    "    for my_image in list_of_images:\n",
    "        image_array = read_image(my_image)\n",
    "        imageindexer.add(my_image, image_array)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
